{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNKJYvxF4SY8Af5D2NSmShp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#1. Import Necessary Libraries"],"metadata":{"id":"dXNMYYdHWpTj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWc3l7nsgfn2"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import Ridge\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","source":["* numpy: For dataset creation and numerical operations.\n","* matplotlib.pyplot: For visualizing results.\n","* PolynomialFeatures: Expands input data into polynomial terms to simulate model complexity.\n","* Ridge: Implements Ridge regression with L2 regularization for controlled overfitting.\n","* mean_squared_error: To calculate the error between predicted and true values.\n","* train_test_split: Splits data into training and testing sets.\n"],"metadata":{"id":"id41noN2hyKV"}},{"cell_type":"markdown","source":["#2. Generate Synthetic Dataset"],"metadata":{"id":"pwNe4eNXWtGN"}},{"cell_type":"code","source":["# Generate synthetic dataset\n","np.random.seed(42)\n","n_samples = 30\n","X = np.random.uniform(-1, 1, size=(n_samples, 1))\n","y = np.sin(2 * np.pi * X).ravel() + 0.3 * np.random.normal(size=n_samples)\n","\n","# Split the dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"],"metadata":{"id":"zrGJAXwrh5Q-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Dataset Creation:\n","  * X: Uniformly sampled points between -1 and 1.\n","  * y: Sine wave values with Gaussian noise (\n","ðœŽ\n","= 0.3) to simulate a noisy regression problem.\n","* Purpose: Mimics real-world data with inherent noise to analyze overfitting and regularization techniques.\n","* Train-Test Split:\n","  * 80% Training Data: Used to train the model.\n","  * 20% Testing Data: Used to evaluate generalization."],"metadata":{"id":"7WAF_frjh71k"}},{"cell_type":"markdown","source":["#3. Define Parameters"],"metadata":{"id":"TtJsTz6AWvc_"}},{"cell_type":"code","source":["# Parameters to explore\n","degrees = np.arange(1, 50)\n","dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.4]\n","alphas = [0.0, 0.1, 0.5, 1.0]  # L2 regularization levels"],"metadata":{"id":"bhe9H3cCiJPl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* degrees: Represents the polynomial degree, which determines model complexity.\n","* dropout_rates: Range of dropout rates to analyze their effect on overfitting and double descent.\n","* alphas: Ridge regression regularization strengths. Higher values enforce stronger weight penalization."],"metadata":{"id":"kFTswECOiLgr"}},{"cell_type":"markdown","source":["#4. Initialize Results Storage"],"metadata":{"id":"idNo8dtHWyGh"}},{"cell_type":"code","source":["# Initialize data storage for results\n","results = {}"],"metadata":{"id":"IoPHJkuVjZFf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* A nested dictionary to store:\n","  * train_errors: Training errors for each combination of dropout and L2 regularization.\n","  * test_errors: Testing errors to assess generalization."],"metadata":{"id":"As-DvKgXjbV9"}},{"cell_type":"markdown","source":["#5. Compute Results for Dropout and L2 Regularization"],"metadata":{"id":"YKmIFuKgW0PQ"}},{"cell_type":"code","source":["for alpha in alphas:\n","    results[alpha] = {}\n","    for dropout_rate in dropout_rates:\n","        train_errors = []\n","        test_errors = []\n","        for degree in degrees:\n","            # Polynomial features\n","            poly = PolynomialFeatures(degree=degree)\n","            X_train_poly = poly.fit_transform(X_train)\n","            X_test_poly = poly.transform(X_test)\n","\n","            # Apply dropout by randomly zeroing out features\n","            if dropout_rate > 0:\n","                mask = np.random.binomial(1, 1 - dropout_rate, X_train_poly.shape)\n","                X_train_poly = X_train_poly * mask\n","\n","            # Ridge regression model\n","            model = Ridge(alpha=alpha)\n","            model.fit(X_train_poly, y_train)\n","\n","            # Calculate errors\n","            y_train_pred = model.predict(X_train_poly)\n","            y_test_pred = model.predict(X_test_poly)\n","            train_errors.append(mean_squared_error(y_train, y_train_pred))\n","            test_errors.append(mean_squared_error(y_test, y_test_pred))\n","\n","        # Store errors for current combination\n","        results[alpha][dropout_rate] = {\n","            \"train_errors\": train_errors,\n","            \"test_errors\": test_errors,\n","        }"],"metadata":{"id":"UGbTB9EIjggK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Outer Loops:\n","* Iterate over all combinations of L2 regularization (alphas) and dropout rates (dropout_rates).\n","2. Inner Loop:\n","* Polynomial Feature Expansion:\n","Transforms input data into higher-dimensional space based on the degree.\n","* Dropout Simulation:\n","Randomly zeroes out features using a binary mask.\n","* Ridge Regression:\n","Fits the model using the regularized least squares approach.\n","* Error Calculation:\n","Compute Mean Squared Error (MSE) for both training and testing datasets.\n","3. Store Results:\n","* Errors for each combination are stored for later analysis and visualization."],"metadata":{"id":"dXdPfGHEjlKc"}},{"cell_type":"markdown","source":["#6. Visualize Results"],"metadata":{"id":"T8W2SOxqW2_i"}},{"cell_type":"code","source":["# Plot results\n","fig, axes = plt.subplots(len(alphas), 1, figsize=(10, 10), sharex=True)\n","for i, alpha in enumerate(alphas):\n","    ax = axes[i]\n","    for dropout_rate in dropout_rates:\n","        test_errors = results[alpha][dropout_rate][\"test_errors\"]\n","        ax.plot(\n","            degrees,\n","            test_errors,\n","            label=f\"Dropout={dropout_rate}\",\n","            marker=\"o\",\n","            linestyle=\"-\",\n","        )\n","    ax.set_yscale(\"log\")\n","    ax.set_title(f\"L2 Regularization (Alpha={alpha})\")\n","    ax.set_xlabel(\"Model Complexity (Polynomial Degree)\")\n","    ax.set_ylabel(\"MSE (Log Scale)\")\n","    ax.legend()\n","    ax.grid()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"teL28OOrjgoC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Subplots:\n","* Each subplot represents a different L2 regularization strength (alpha).\n","* Dropout rates are compared within each subplot.\n","2. Plotting:\n","* X-Axis: Polynomial degree, representing model complexity.\n","* Y-Axis: Mean squared error in logarithmic scale.\n","3. Annotations:\n","* Titles, labels, and legends highlight L2 regularization effects and dropout rate variations.\n"],"metadata":{"id":"asDrEP6Lj-aD"}}]}