{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOTxBTZofdg/jdATNS1huPX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Install required packages\n","\n","!pip install tensorflow==2.18.0\n","!pip install keras==3.7.0\n","!pip install torch==2.5.1\n","!pip install torchvision==0.20.1\n","\n","!pip install numpy==2.0.2\n","!pip install scipy==1.14.1\n","!pip install pandas==2.2.3\n","\n","!pip install scikit-learn==1.5.2\n","\n","!pip install matplotlib==3.9.2\n","\n","!pip install joblib==1.4.2\n","!pip install python-dateutil==2.9.0.post0\n","\n","!pip install sympy==1.13.1\n","!pip install opt-einsum==3.4.0\n","\n","!pip install tensorboard==2.18.0\n","!pip install protobuf==5.29.0\n","!pip install threadpoolctl==3.5.0\n","!pip install packaging==24.2\n"],"metadata":{"id":"f0J0TogGzzRe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#1. Import Required Libraries"],"metadata":{"id":"d8P74yKoXykL"}},{"cell_type":"markdown","source":["This section implements a custom ResNet (Residual Network) model with adjustable layers and depths. The code includes:\n","\n","1. BasicBlock for building residual connections.\n","\n","2. ResNet Class with configurable depth using dynamic layers.\n","3. Training and Testing Functions to evaluate model performance on CIFAR-10 dataset.\n","4. Experiments to compare ResNet variants (Small to XXX-Large) to observe the effect of increasing model complexity on training and test losses.\n"],"metadata":{"id":"HD_bGXO7tCPW"}},{"cell_type":"code","source":["# Import Required Libraries\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np"],"metadata":{"id":"qe0Y_w5bt18k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This block imports the essential libraries required for:\n","\n","torch and torch.nn: Used for defining and training neural network models.\n","\n","torchvision: Provides the CIFAR-10 dataset and prebuilt transformations.\n","\n","matplotlib.pyplot: Used for visualizing training and testing losses.\n","\n","numpy: Utility library for numerical operations.\n"],"metadata":{"id":"3WG6amUmt-Nc"}},{"cell_type":"markdown","source":["#2. Define BasicBlock"],"metadata":{"id":"dnu-bYK-X5-3"}},{"cell_type":"code","source":["# Define BasicBlock for ResNet\n","class BasicBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.shortcut = nn.Sequential()\n","\n","        if stride != 1 or in_channels != out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","\n","    def forward(self, x):\n","        out = torch.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = torch.relu(out)\n","        return out"],"metadata":{"id":"woSJMDKKuK3H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The BasicBlock class implements a residual block, the fundamental building block of ResNet. It includes:\n","\n","* Two convolutional layers with Batch Normalization, which ensures stable training by normalizing activations.\n","* A shortcut connection directly bypassing the input to the output, allowing gradients to flow unimpeded, which addresses gradient vanishing issues in deep networks.\n","* The block supports downsampling when the stride is greater than 1 or the input/output channels differ.\n"],"metadata":{"id":"D8IzQ370uPPw"}},{"cell_type":"markdown","source":["#3. Define ResNet Class"],"metadata":{"id":"yvi08vV6X9-0"}},{"cell_type":"code","source":["# Define ResNet with adjustable layers\n","class ResNet(nn.Module):\n","    def __init__(self, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n","        self.bn1 = nn.BatchNorm2d(16)\n","        self.in_channels = 16  # Start with 16 channels\n","\n","        # Create layers dynamically\n","        self.layer1 = self._make_layer(16, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(32, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(64, num_blocks[2], stride=2)\n","\n","        # Fully connected layer initialized dynamically\n","        self.fc = None\n","        self.num_classes = num_classes\n","\n","    def _make_layer(self, out_channels, blocks, stride):\n","        layers = []\n","        layers.append(BasicBlock(self.in_channels, out_channels, stride))  # First block\n","        self.in_channels = out_channels  # Update the number of input channels\n","        for _ in range(1, blocks):\n","            layers.append(BasicBlock(out_channels, out_channels))  # Remaining blocks\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = torch.relu(self.bn1(self.conv1(x)))  # Initial conv layer\n","        out = self.layer1(out)  # Layer 1\n","        out = self.layer2(out)  # Layer 2\n","        out = self.layer3(out)  # Layer 3\n","        out = torch.nn.functional.avg_pool2d(out, 4)  # Global average pooling\n","\n","        if self.fc is None:\n","            flattened_size = out.view(out.size(0), -1).size(1)\n","            self.fc = nn.Linear(flattened_size, self.num_classes).to(out.device)\n","\n","        out = out.view(out.size(0), -1)  # Flatten\n","        out = self.fc(out)  # Fully connected layer\n","        return out"],"metadata":{"id":"DnL-4EPJuwf7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The ResNet class builds the overall network using BasicBlock. Key components:\n","\n","* An initial convolutional layer to process the input.\n","* Layers dynamically built from BasicBlock, where:\n","   * The number of channels doubles in each subsequent layer.\n","   * The stride is adjusted to downsample the feature maps.\n","* A global average pooling layer reduces the feature map size before flattening.\n","* A fully connected layer (fc) computes the final class probabilities."],"metadata":{"id":"XFqlDSN9JuXa"}},{"cell_type":"markdown","source":["#4. Dataset and DataLoader"],"metadata":{"id":"B1kiONlIYGSV"}},{"cell_type":"code","source":["# Dataset and DataLoader\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)"],"metadata":{"id":"Xw5fM3QDKHl6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The CIFAR-10 dataset is used for this experiment. This block:\n","\n","1. Applies data normalization to scale pixel values to a range centered at 0.\n","2. Creates training and testing datasets using torchvision's datasets.CIFAR10.\n","3. Wraps the datasets in PyTorch's DataLoader, which facilitates batch processing and shuffling for efficient training and evaluation.\n"],"metadata":{"id":"9LCuoamcKNe-"}},{"cell_type":"markdown","source":["#5. Training Function (train)"],"metadata":{"id":"Qq-b4ya1YItA"}},{"cell_type":"code","source":["# Training Function\n","def train(model, device, trainloader, optimizer, criterion, epoch):\n","    model.train()\n","    total_loss = 0\n","    for inputs, targets in trainloader:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(trainloader)\n","\n","# Testing Function\n","def test(model, device, testloader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for inputs, targets in testloader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            total_loss += loss.item()\n","    return total_loss / len(testloader)"],"metadata":{"id":"ZvSBDqgeKagn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This function trains the model on the CIFAR-10 training set:\n","\n","* Switches the model to training mode using model.train().\n","* Iterates over batches of input data and their labels.\n","* Computes predictions, calculates the loss using a predefined criterion (e.g., Cross-Entropy Loss), and updates model weights via backpropagation.\n","\n","The average loss across all batches is returned.\n","\n","Next function evaluates the model's performance on the CIFAR-10 test set:\n","\n","* Switches the model to evaluation mode using model.eval(), ensuring no dropout or batch normalization updates occur during inference.\n","* Computes predictions and calculates the loss for each batch without updating weights (using torch.no_grad() for efficiency).\n","* Returns the average loss across all test batches."],"metadata":{"id":"4eFh3TxhKeL3"}},{"cell_type":"markdown","source":["#6. Train and Evaluate Models"],"metadata":{"id":"j-d8a7c0YLdi"}},{"cell_type":"code","source":["# Train and Evaluate Models\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model_variants = [\n","    {\"name\": \"Small ResNet\", \"blocks\": [1, 1, 1]},\n","    {\"name\": \"Medium ResNet\", \"blocks\": [2, 2, 2]},\n","    {\"name\": \"Large ResNet\", \"blocks\": [3, 3, 3]},\n","]\n","\n","criterion = nn.CrossEntropyLoss()\n","train_losses, test_losses = [], []\n","\n","for variant in model_variants:\n","    model = ResNet(variant[\"blocks\"]).to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    model_train_losses, model_test_losses = [], []\n","\n","    for epoch in range(1, 21):\n","        train_loss = train(model, device, trainloader, optimizer, criterion, epoch)\n","        test_loss = test(model, device, testloader, criterion)\n","        model_train_losses.append(train_loss)\n","        model_test_losses.append(test_loss)\n","\n","    train_losses.append(model_train_losses)\n","    test_losses.append(model_test_losses)\n","\n","# Plot Results\n","plt.figure(figsize=(12, 8))\n","for idx, variant in enumerate(model_variants):\n","    plt.plot(range(1, 21), train_losses[idx], label=f\"{variant['name']} - Train Loss\")\n","    plt.plot(range(1, 21), test_losses[idx], linestyle=\"--\", label=f\"{variant['name']} - Test Loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.title(\"Training and Testing Loss for ResNet Variants\")\n","plt.show()"],"metadata":{"id":"SMJQLyHBKqcJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This section trains and evaluates multiple ResNet variants (Small, Medium, Large) with varying numbers of blocks:\n","\n","1. For each model variant:\n","   * Initializes a ResNet model with the specified block configuration.\n","  * Trains the model over 20 epochs, recording training and testing losses after each epoch.\n","2. Plots the training and testing losses for all variants to compare their performance and observe how increasing model complexity impacts the results."],"metadata":{"id":"Lzu3KbSGKvge"}}]}