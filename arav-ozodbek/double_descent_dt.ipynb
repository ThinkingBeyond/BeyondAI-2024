{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMvAjKD+d8RGG8JZXxF4z0u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Install required packages\n","\n","!pip install tensorflow==2.18.0\n","!pip install keras==3.7.0\n","!pip install torch==2.5.1\n","!pip install torchvision==0.20.1\n","\n","!pip install numpy==2.0.2\n","!pip install scipy==1.14.1\n","!pip install pandas==2.2.3\n","\n","!pip install scikit-learn==1.5.2\n","\n","!pip install matplotlib==3.9.2\n","\n","!pip install joblib==1.4.2\n","!pip install python-dateutil==2.9.0.post0\n","\n","!pip install sympy==1.13.1\n","!pip install opt-einsum==3.4.0\n","\n","!pip install tensorboard==2.18.0\n","!pip install protobuf==5.29.0\n","!pip install threadpoolctl==3.5.0\n","!pip install packaging==24.2\n"],"metadata":{"id":"OkE0GY0FzuVA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#1. Import Necessary Libraries"],"metadata":{"id":"EMN_R8rHYg1v"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2RwRCvz4ob2"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","source":["* numpy: For numerical computations, particularly for generating and manipulating arrays.\n","\n","* matplotlib.pyplot: Used for plotting graphs, such as loss curves or complexity analysis.\n","\n","* DecisionTreeRegressor: Implements a decision tree model for regression tasks.\n","\n","* mean_squared_error: Calculates the error between predicted and true values.\n","\n","* train_test_split: Splits the dataset into training and testing subsets to evaluate model performance."],"metadata":{"id":"pXEVUId34x9J"}},{"cell_type":"markdown","source":["#2. Generate Synthetic Dataset"],"metadata":{"id":"4UbfmtzVYiuz"}},{"cell_type":"code","source":["# Generate synthetic dataset\n","np.random.seed(42)\n","n_samples = 100  # Increased dataset size for reliability\n","X = np.random.uniform(-1, 1, size=(n_samples, 1))\n","y = np.sin(2 * np.pi * X).ravel() + 1 * np.random.normal(size=n_samples)  # Reduced noise"],"metadata":{"id":"Gq9-J1Me5L6x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* np.random.seed(42): Sets a fixed seed for reproducibility.\n","* Dataset:\n","   * X: Input data, uniformly distributed between -1 and 1.\n","   * y: Output data generated using a sine function, with added Gaussian noise (\n","ùúé\n","=\n","1).\n","   * Purpose: Mimics a regression problem, where a noisier dataset adds realism."],"metadata":{"id":"I0bkfqVSIZN-"}},{"cell_type":"markdown","source":["#3. Split the Dataset"],"metadata":{"id":"RObeba2wYlso"}},{"cell_type":"code","source":["# Split the dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 10, random_state=42)\n"],"metadata":{"id":"3OxfONn3IrGJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Splits the data into training and testing sets.\n","    * Training set: Used to fit the model.\n","    * Testing set: Used to evaluate the model's generalization.\n","* test_size=10: Ensures the test set contains 10 samples.\n","* random_state=42: Ensures reproducibility of splits."],"metadata":{"id":"HlEQxJzoJLR-"}},{"cell_type":"markdown","source":["#4. Initialize Storage for Results"],"metadata":{"id":"hT_fqOLgYn_y"}},{"cell_type":"code","source":["# Initialize arrays for results\n","train_errors = []\n","test_errors = []\n","degrees = np.arange(1, 200)  # Model complexity: Polynomial degrees"],"metadata":{"id":"fLb0yOh3JagH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* train_errors and test_errors: Lists to store mean squared errors for training and testing datasets across polynomial degrees.\n","* degrees: Specifies the range of polynomial degrees to test, increasing model complexity incrementally from 1 to 199.\n"],"metadata":{"id":"daRY1N9VKBw0"}},{"cell_type":"markdown","source":["#5. Iterate Over Polynomial Degrees"],"metadata":{"id":"SIuMhMazYqas"}},{"cell_type":"code","source":["# Iterate over polynomial degrees\n","for degree in degrees:\n","    poly = PolynomialFeatures(degree=degree)\n","    X_train_poly = poly.fit_transform(X_train)\n","    X_test_poly = poly.transform(X_test)\n","\n","    # Fit polynomial regression model with regularization\n","    model = Ridge(alpha=0.0)  # Small regularization to stabilize\n","    model.fit(X_train_poly, y_train)\n","\n","    # Calculate train and test errors\n","    y_train_pred = model.predict(X_train_poly)\n","    y_test_pred = model.predict(X_test_poly)\n","    train_errors.append(mean_squared_error(y_train, y_train_pred))\n","    test_errors.append(mean_squared_error(y_test, y_test_pred))"],"metadata":{"id":"Kzxux0YYKM2r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1.Data Transformation:\n","\n","* PolynomialFeatures: Expands input features into polynomial terms of the specified degree.\n","* fit_transform: Fits and transforms the training data.\n","* transform: Applies the same transformation to the test data.\n","2. Model Training:\n","\n","* Ridge(alpha=0.0): Performs linear regression with no additional regularization. This helps stabilize the polynomial regression but does not heavily penalize large coefficients.\n","3. Predictions:\n","\n","* y_train_pred: Predictions on the training set.\n","* y_test_pred: Predictions on the test set.\n","4. Error Calculation:\n","\n","* Mean squared errors for both training and test sets are computed and stored."],"metadata":{"id":"7AE9dvC7KR2H"}},{"cell_type":"markdown","source":["#6. Visualize Results"],"metadata":{"id":"RVAir_0UYs4y"}},{"cell_type":"code","source":["# Plot results\n","plt.figure(figsize=(10, 6))\n","plt.plot(degrees, train_errors, label='Train Loss', marker='o')\n","plt.plot(degrees, test_errors, label='Test Loss', marker='o')\n","plt.yscale('log')\n","plt.xlabel('Model Complexity (Polynomial Degree)')\n","plt.ylabel('Mean Squared Error (Log Scale)')\n","plt.title('Double Descent in Polynomial Regression (Improved Setup)')\n","plt.legend()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"e5rp-qXPKylz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot Setup:\n","\n","X-Axis: Polynomial degree (model complexity).\n","Y-Axis: Mean squared error in logarithmic scale for better visualization of trends.\n","Data Representation:\n","\n","train_errors: Plotted as a line with markers to show the training loss at each degree.\n","test_errors: Plotted similarly for test loss.\n","Title and Labels: Clearly describe the relationship between model complexity and generalization performance.\n","\n","Log Scale: Helps highlight smaller variations in mean squared errors."],"metadata":{"id":"n0SLnQb3K2Pr"}}]}