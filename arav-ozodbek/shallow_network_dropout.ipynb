{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM4/QRPdf8UXo5lx1M55Wig"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#1. Import Necessary Libraries"],"metadata":{"id":"bnTQGWp0CcvN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"i67k4n_dAa0i"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","from keras.optimizers import Adam"]},{"cell_type":"markdown","source":["* numpy: For numerical computations and dataset generation.\n","* matplotlib.pyplot: For visualizing loss and accuracy trends.\n","* train_test_split: Splits the dataset into training and testing subsets.\n","* Sequential, Dense, Dropout: Keras tools to construct a shallow neural network with dropout.\n","* Adam: Optimizer to train the model efficiently."],"metadata":{"id":"xcCTiEWyAwEi"}},{"cell_type":"markdown","source":["#2. Generate Synthetic Dataset"],"metadata":{"id":"Ys4FZKgQCnTu"}},{"cell_type":"code","source":["np.random.seed(42)\n","n_samples = 100  # Larger dataset for better training\n","X = np.random.uniform(-1, 1, size=(n_samples, 1))\n","y = (np.sin(2 * np.pi * X).ravel() + 0.7 * np.random.normal(size=n_samples)) > 0  # Binary classification"],"metadata":{"id":"WbB8hU0zA3-s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* X: Input features sampled uniformly between -1 and 1.\n","* y: Binary labels created by thresholding a sine function with added Gaussian noise (\n","ùúé\n","= 0.7)."],"metadata":{"id":"ZnIiNhbNA5LJ"}},{"cell_type":"markdown","source":["#3. Split Data into Training and Testing Sets"],"metadata":{"id":"omXms1VxCt10"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"Jnhj1aCvBAVf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Training Set: 80% of the data for model training.\n","* Testing Set: 20% of the data for evaluating the model."],"metadata":{"id":"d4eXqnFYBCpi"}},{"cell_type":"markdown","source":["#4. Define Experiment Configurations"],"metadata":{"id":"D9fHzV7QCx8O"}},{"cell_type":"code","source":["epoch_counts = [250, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500]  # Epoch range\n","dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.4]  # Dropout regularization rates\n","hidden_layer_size = 50  # Number of neurons in the hidden layer"],"metadata":{"id":"yo9yM7QUBHuj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* epoch_counts: Specifies the range of training durations to analyze the impact of epochs.\n","* dropout_rates: Defines different levels of dropout regularization (from no dropout to 40%).\n","* hidden_layer_size: Fixes the size of the hidden layer."],"metadata":{"id":"XJJSnIfYBKYE"}},{"cell_type":"markdown","source":["#5. Initialize Results Storage"],"metadata":{"id":"LfZsDJZ5C3S3"}},{"cell_type":"code","source":["results = {}"],"metadata":{"id":"iQDLzOO4BPP1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A dictionary to store training and testing losses and accuracies for each dropout rate."],"metadata":{"id":"Ya3rEgpKBRbw"}},{"cell_type":"markdown","source":["#6. Loop Over Dropout Rates and Epoch Configurations"],"metadata":{"id":"CfrqDRqkC7Z6"}},{"cell_type":"code","source":["for dropout_rate in dropout_rates:\n","    train_losses, test_losses = [], []\n","    train_accuracies, test_accuracies = [], []\n","\n","    for epochs in epoch_counts:\n","        # Build the model\n","        model = Sequential([\n","            Dense(hidden_layer_size, input_dim=1, activation='relu'),\n","            Dropout(dropout_rate),  # Dropout layer\n","            Dense(1, activation='sigmoid')\n","        ])\n","\n","        # Compile the model\n","        model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","        # Train the model\n","        model.fit(X_train, y_train, epochs=epochs, verbose=0, batch_size=16)\n","\n","        # Evaluate the model\n","        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n","        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n","\n","        # Log the results\n","        train_losses.append(train_loss)\n","        test_losses.append(test_loss)\n","        train_accuracies.append(train_acc)\n","        test_accuracies.append(test_acc)\n","\n","    # Store results for the current dropout rate\n","    results[dropout_rate] = {\n","        'train_losses': train_losses,\n","        'test_losses': test_losses,\n","        'train_accuracies': train_accuracies,\n","        'test_accuracies': test_accuracies\n","    }"],"metadata":{"id":"XNCt0A4sBTfi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Model Creation:\n","\n","* A shallow neural network is created with a dropout layer applied after the hidden layer.\n","* The dropout rate is varied across experiments.\n","2. Compilation:\n","\n","* Optimizer: Adam with a learning rate of 0.01.\n","* Loss: Binary Crossentropy for binary classification.\n","* Metrics: Accuracy is tracked during evaluation.\n","3. Training and Evaluation:\n","\n","* The model is trained for each epoch configuration.\n","* Training and testing losses and accuracies are recorded.\n","4. Result Storage:\n","\n","* Results are stored for each dropout rate, including training and testing metrics."],"metadata":{"id":"YdoUS_5vBXy5"}},{"cell_type":"markdown","source":["#7. Visualize Loss Results"],"metadata":{"id":"EhPchK3QC_A2"}},{"cell_type":"code","source":["plt.figure(figsize=(10, 6))\n","for dropout_rate in dropout_rates:\n","    plt.plot(epoch_counts, results[dropout_rate]['test_losses'], label=f\"Test Loss (Dropout={dropout_rate})\", marker='o')\n","    plt.plot(epoch_counts, results[dropout_rate]['train_losses'], linestyle='--', label=f\"Train Loss (Dropout={dropout_rate})\")\n","\n","plt.xlabel('Number of Epochs')\n","plt.ylabel('Loss (Binary Crossentropy)')\n","plt.title('Effect of Dropout Regularization on Loss')\n","plt.legend()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"_Xrn4HlnBtUY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["X-axis: Number of epochs.\n","\n","Y-axis: Binary crossentropy loss (train and test).\n","\n","Curves:\n","* Solid lines for test loss.\n","* Dashed lines for training loss."],"metadata":{"id":"9oG_yqkXBwOh"}},{"cell_type":"markdown","source":["#8. Visualize Accuracy Results"],"metadata":{"id":"ZJB4zaEkDDhE"}},{"cell_type":"code","source":["plt.figure(figsize=(10, 6))\n","for dropout_rate in dropout_rates:\n","    plt.plot(epoch_counts, results[dropout_rate]['test_accuracies'], label=f\"Test Accuracy (Dropout={dropout_rate})\", marker='o')\n","    plt.plot(epoch_counts, results[dropout_rate]['train_accuracies'], linestyle='--', label=f\"Train Accuracy (Dropout={dropout_rate})\")\n","\n","plt.xlabel('Number of Epochs')\n","plt.ylabel('Accuracy')\n","plt.title('Effect of Dropout Regularization on Accuracy')\n","plt.legend()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"hwkFqIpdB4b5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["X-axis: Number of epochs.\n","\n","Y-axis: Accuracy (train and test).\n","\n","Curves:\n","\n","* Solid lines for test accuracy.\n","* Dashed lines for training accuracy."],"metadata":{"id":"jchqnWjVB6oP"}},{"cell_type":"markdown","source":["#Key Observations\n","Dropout Impact:\n","\n","* Reduces overfitting at moderate rates (e.g., 0.1‚Äì0.2).\n","* High dropout rates (e.g., 0.4) may cause underfitting, leading to increased test loss.\n","\n","Double Descent:\n","\n","* Loss and accuracy trends highlight epoch-wise double descent behavior across dropout configurations."],"metadata":{"id":"OevIFL7RCGAy"}}]}