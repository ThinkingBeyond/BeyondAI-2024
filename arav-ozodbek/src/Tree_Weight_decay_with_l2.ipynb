{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyORFsHAtfKll0mjHVLJbU+T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Install required packages\n","\n","!pip install tensorflow==2.18.0\n","!pip install keras==3.7.0\n","!pip install torch==2.5.1\n","!pip install torchvision==0.20.1\n","\n","!pip install numpy==2.0.2\n","!pip install scipy==1.14.1\n","!pip install pandas==2.2.3\n","\n","!pip install scikit-learn==1.5.2\n","\n","!pip install matplotlib==3.9.2\n","\n","!pip install joblib==1.4.2\n","!pip install python-dateutil==2.9.0.post0\n","\n","!pip install sympy==1.13.1\n","!pip install opt-einsum==3.4.0\n","\n","!pip install tensorboard==2.18.0\n","!pip install protobuf==5.29.0\n","!pip install threadpoolctl==3.5.0\n","!pip install packaging==24.2\n"],"metadata":{"id":"gN_ihPqG0Py9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#1. Import Necessary Libraries"],"metadata":{"id":"4_Rje_TSVZp5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMCLnyLhssPk"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import Ridge\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","source":["* numpy: *Used for numerical operations and dataset creation*.\n","* matplotlib.pyplot: *For visualizing MSE and $R^2$ scores*.\n","* PolynomialFeatures: *Expands input features into polynomial terms for model complexity adjustment*.\n","* Ridge: *Implements Ridge regression with L2 regularization to reduce overfitting.*\n","* mean_squared_error and r2_score: *Metrics to evaluate regression performance.*\n","* train_test_split: *Splits the dataset into training and testing sets.*\n"],"metadata":{"id":"KPBstQetvCIZ"}},{"cell_type":"markdown","source":["#2. Generate Synthetic Dataset"],"metadata":{"id":"LLCiZbZvVjDH"}},{"cell_type":"code","source":["# Generate synthetic dataset\n","np.random.seed(42)\n","n_samples = 30  # Dataset size\n","X = np.random.uniform(-1, 1, size=(n_samples, 1))\n","y = np.sin(2 * np.pi * X).ravel() + 0.3 * np.random.normal(size=n_samples)"],"metadata":{"id":"3O6GOputvWgY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* **X**: Input features are *uniformly sampled in the range [-1, 1].*\n","* **y**: Output labels are based on a sine function with added Gaussian noise (\n","  *ùúé = 0.3* ), simulating real-world noisy data."],"metadata":{"id":"tbmktSqGvXhr"}},{"cell_type":"markdown","source":["#3. Define Parameters"],"metadata":{"id":"XMV6TBa8VmdD"}},{"cell_type":"code","source":["# Weight decay values to analyze (L2 regularization strength)\n","weight_decay_values = [0.0, 0.1, 0.5, 1.0, 5.0]\n","\n","# Initialize results\n","degrees = np.arange(1, 50)  # Model complexity: Polynomial degrees\n","results = {wd: {'train_errors': [], 'test_errors': [], 'train_r2': [], 'test_r2': []}\n","           for wd in weight_decay_values}"],"metadata":{"id":"HPB7kqX0vuZe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* **weight_decay_values**: Specifies the *L2 regularization strength* (**alpha parameter** in Ridge regression). *Values range from no regularization (0.0) to strong regularization (5.0)*.\n","* **degrees**: *Represents polynomial degrees, controlling model complexity*.\n","* **results**: *Dictionary structure* to store MSE and $R^2$\n","  scores for training and testing datasets across different weight decay values."],"metadata":{"id":"7S27eZWVvxgN"}},{"cell_type":"markdown","source":["#4. Iterate Over Weight Decay Values and Compute Results"],"metadata":{"id":"RuF9VCY2VpUK"}},{"cell_type":"code","source":["# Iterate over weight decay values\n","for weight_decay in weight_decay_values:\n","    train_errors = []\n","    test_errors = []\n","    train_r2_scores = []\n","    test_r2_scores = []\n","\n","    # Split data\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n","\n","    for degree in degrees:\n","        poly = PolynomialFeatures(degree=degree)\n","        X_train_poly = poly.fit_transform(X_train)\n","        X_test_poly = poly.transform(X_test)\n","\n","        # Fit polynomial regression model with weight decay (Ridge regression)\n","        model = Ridge(alpha=weight_decay)  # Alpha controls the weight decay strength\n","        model.fit(X_train_poly, y_train)\n","\n","        # Calculate train and test errors\n","        y_train_pred = model.predict(X_train_poly)\n","        y_test_pred = model.predict(X_test_poly)\n","\n","        train_errors.append(mean_squared_error(y_train, y_train_pred))\n","        test_errors.append(mean_squared_error(y_test, y_test_pred))\n","        train_r2_scores.append(r2_score(y_train, y_train_pred))\n","        test_r2_scores.append(r2_score(y_test, y_test_pred))\n","\n","    results[weight_decay]['train_errors'] = train_errors\n","    results[weight_decay]['test_errors'] = test_errors\n","    results[weight_decay]['train_r2'] = train_r2_scores\n","    results[weight_decay]['test_r2'] = test_r2_scores"],"metadata":{"id":"ZZ4KDgrxv9v-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Outer Loop:\n","* *Iterates over all L2 regularization values*.\n","2. Inner Loop:\n","\n","For each **polynomial degree**:\n","1. **Feature expansion**:\n","* *Expands features into polynomial terms* using **PolynomialFeatures**.\n","2. **Model training**:\n","* Ridge regression (*with the current weight decay value*) is used to fit the training data.\n","\n","3. **Performance metrics**:\n","* mean_squared_error: *Quantifies the error in predictions*.\n","* r2_score: *Measures how well the model explains variance in the data*.\n","\n","Results are *appended to lists* for later visualization."],"metadata":{"id":"TkT6C45mv_6R"}},{"cell_type":"markdown","source":["#5. Visualize Mean Squared Error (MSE) Results"],"metadata":{"id":"AXyVWI7mVstk"}},{"cell_type":"code","source":["# Plot MSE results\n","plt.figure(figsize=(12, 8))\n","for weight_decay in weight_decay_values:\n","    plt.plot(degrees, results[weight_decay]['test_errors'], label=f\"Test Loss (Weight Decay={weight_decay})\", marker='o')\n","    plt.plot(degrees, results[weight_decay]['train_errors'], linestyle='--', label=f\"Train Loss (Weight Decay={weight_decay})\")\n","\n","plt.xlabel(\"Model Complexity (Polynomial Degree)\")\n","plt.ylabel(\"Mean Squared Error (Log Scale)\")\n","plt.yscale(\"log\")\n","plt.title(\"Effect of Weight Decay Regularization on Double Descent (MSE)\")\n","plt.legend()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"MPlTYj6KwU0Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**MSE Plot**\n","* **Purpose**: *Plots training and testing MSE against model complexity for different weight decay values*.\n","* **Logarithmic scaling**: *Helps visualize differences across orders of magnitude*.\n","* **Line Representation**:\n","   * Dashed lines represent *training loss*.\n","   * Solid lines represent *test loss*."],"metadata":{"id":"mIqZr2I6wXV-"}},{"cell_type":"markdown","source":["#6. Visualize ùëÖ^2 Score Results"],"metadata":{"id":"yemeVZGNVv8m"}},{"cell_type":"code","source":["# Plot R2 results\n","plt.figure(figsize=(12, 8))\n","for weight_decay in weight_decay_values:\n","    plt.plot(degrees, results[weight_decay]['test_r2'], label=f\"Test R¬≤ (Weight Decay={weight_decay})\", marker='o')\n","    plt.plot(degrees, results[weight_decay]['train_r2'], linestyle='--', label=f\"Train R¬≤ (Weight Decay={weight_decay})\")\n","\n","plt.xlabel(\"Model Complexity (Polynomial Degree)\")\n","plt.ylabel(\"R¬≤ Score\")\n","plt.title(\"Effect of Weight Decay Regularization on Double Descent (R¬≤)\")\n","plt.legend()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"REh3TdYywdw8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["$R^2$ Plot\n","* **Purpose**: *Visualizes $R^2$\n","  scores for training and testing data*, indicating the proportion of variance explained by the model.\n","* **Highlights**: *Shows how weight decay influences generalization performance across different polynomial complexities*."],"metadata":{"id":"j0M9bByLwgUA"}},{"cell_type":"markdown","source":["# Key Takeaways\n","* Effect of Weight Decay:\n","   * **Higher weight decay values** *smooth the test loss curve, mitigating overfitting but potentially underfitting simpler data*.\n","   * **Moderate weight decay** *balances overfitting and underfitting*, achieving stable test performance."],"metadata":{"id":"iEu4o6DUwsHU"}}]}