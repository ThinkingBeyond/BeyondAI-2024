{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM2HirdYMYU1snbtj+FZaiV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Install required packages\n","\n","!pip install tensorflow==2.18.0\n","!pip install keras==3.7.0\n","!pip install torch==2.5.1\n","!pip install torchvision==0.20.1\n","\n","!pip install numpy==2.0.2\n","!pip install scipy==1.14.1\n","!pip install pandas==2.2.3\n","\n","!pip install scikit-learn==1.5.2\n","\n","!pip install matplotlib==3.9.2\n","\n","!pip install joblib==1.4.2\n","!pip install python-dateutil==2.9.0.post0\n","\n","!pip install sympy==1.13.1\n","!pip install opt-einsum==3.4.0\n","\n","!pip install tensorboard==2.18.0\n","!pip install protobuf==5.29.0\n","!pip install threadpoolctl==3.5.0\n","!pip install packaging==24.2\n"],"metadata":{"id":"9PLudLtj0TJN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#1. Import Necessary Libraries"],"metadata":{"id":"DKkGvU8iQua0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ppnhZ9CVxOKr"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.optimizers import Adam\n","from keras.regularizers import l2"]},{"cell_type":"markdown","source":["* numpy: For numerical computations and dataset creation.\n","* matplotlib.pyplot: For visualizing loss behavior.\n","* train_test_split: Splits the dataset into training and testing sets.\n","* Sequential, Dense: Keras tools to build a shallow neural network model.\n","* Adam: Optimizer for training the model efficiently.\n","* l2: Implements weight decay (L2 regularization) to control overfitting."],"metadata":{"id":"5uK3TtE57x8a"}},{"cell_type":"markdown","source":["#2. Generate Synthetic Dataset"],"metadata":{"id":"Z8WjO_0EQxgy"}},{"cell_type":"code","source":["np.random.seed(42)\n","n_samples = 100  # Larger dataset for better training\n","X = np.random.uniform(-1, 1, size=(n_samples, 1))\n","y = (np.sin(2 * np.pi * X).ravel() + 0.7 * np.random.normal(size=n_samples)) > 0  # Binary classification"],"metadata":{"id":"cnkGo_vr7-Q-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* **X**: Input features sampled uniformly from [*-1, 1*].\n","* **y**: Output labels generated by thresholding a sine function with **Gaussian** noise (*ùúé = 0.7*), creating a **binary classification** problem."],"metadata":{"id":"2uSWohGY8BeG"}},{"cell_type":"markdown","source":["#3. Split Data into Training and Testing Sets"],"metadata":{"id":"CCK6wi5jQ0uV"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"vS0FFFDN8LPu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Splits the dataset into:\n","* **X_train, y_train**: Training data (*80% of the dataset*).\n","* **X_test, y_test**: Testing data (*20% of the dataset*)."],"metadata":{"id":"hNvHLUNn8OBG"}},{"cell_type":"markdown","source":["#4. Define Experiment Configurations"],"metadata":{"id":"bsZSSHxQQ3Kv"}},{"cell_type":"code","source":["epoch_counts = [250, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500]  # Training durations\n","hidden_layer_size = 50  # Size of hidden layer (model complexity)\n","l2_weight = 0.00  # L2 regularization strength (disabled in this experiment)"],"metadata":{"id":"BJ3P30WQ8WUC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* **epoch_counts**: Defines a range of training durations to explore epoch-wise double descent behavior.\n","* **hidden_layer_size**: Sets the number of neurons in the hidden layer, controlling model complexity.\n","* **l2_weight**: Specifies the strength of L2 regularization (*set to **0.00** to observe baseline behavior*)."],"metadata":{"id":"-WTWzQz88ZSW"}},{"cell_type":"markdown","source":["#5. Initialize Result Storage"],"metadata":{"id":"9fDIkxTUVJxf"}},{"cell_type":"code","source":["train_losses = []\n","test_losses = []"],"metadata":{"id":"PpygkOyQ8hgq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* **train_losses**: Stores training losses for each epoch configuration.\n","* **test_losses**: Stores testing losses for each epoch configuration."],"metadata":{"id":"zoNTTrbq8kHU"}},{"cell_type":"markdown","source":["#6. Loop Over Epoch Configurations"],"metadata":{"id":"bTqPyIn7VM-s"}},{"cell_type":"code","source":["for epochs in epoch_counts:\n","    # Build the model\n","    model = Sequential([\n","        Dense(hidden_layer_size, input_dim=1, activation='relu', kernel_regularizer=l2(l2_weight)),\n","        Dense(1, activation='sigmoid', kernel_regularizer=l2(l2_weight))\n","    ])\n","\n","    # Compile the model\n","    model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    # Train the model\n","    model.fit(X_train, y_train, epochs=epochs, verbose=0, batch_size=16)\n","\n","    # Evaluate the model\n","    train_loss, _ = model.evaluate(X_train, y_train, verbose=0)\n","    test_loss, _ = model.evaluate(X_test, y_test, verbose=0)\n","\n","    # Log the results\n","    train_losses.append(train_loss)\n","    test_losses.append(test_loss)"],"metadata":{"id":"hFBryRwy8ovf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Model Creation:\n","\n","* A shallow network with one hidden layer (*hidden_layer_size neurons*) using **ReLU activation**.\n","* Output layer uses a **sigmoid activation** for binary classification.\n","* L2 regularization (*weight decay*) is applied with the configured strength.\n","2. Compilation:\n","\n","* **Optimizer**: Adam with a *learning rate of 0.01*.\n","* **Loss**: Binary Crossentropy, suited for binary classification tasks.\n","* **Metrics**: Accuracy to monitor performance.\n","3. Training:\n","\n","* The model is trained for the specified number of epochs (*epochs*), with a batch size of 16.\n","4. Evaluation:\n","\n","* Calculates train loss and test loss for the current epoch configuration.\n","5. Logging Results:\n","\n","* Stores *training and testing losses* in **train_losses and test_losses** for later visualization."],"metadata":{"id":"XfAJ8xmD8rc0"}},{"cell_type":"markdown","source":["#7. Visualize Results"],"metadata":{"id":"LDwAqoUSVQCy"}},{"cell_type":"code","source":["plt.figure(figsize=(10, 6))\n","plt.plot(epoch_counts, train_losses, label='Train Loss', marker='o')\n","plt.plot(epoch_counts, test_losses, label='Test Loss', marker='o')\n","plt.xlabel('Number of Epochs')\n","plt.ylabel('Loss (Binary Crossentropy)')\n","plt.title('Epoch-wise Double Descent Behavior')\n","plt.legend()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"8KCtaMai9Tms"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* **X-axis**: Number of training epochs.\n","* **Y-axis**: Loss (*Binary Crossentropy*).\n","* **Curves**:\n","  * Training loss (*train_losses*): **Dashed curve**.\n","  * Testing loss (*test_losses*): **Solid curve**.\n","* **Highlights**: *Shows epoch-wise double descent behavior, where test loss decreases, increases, and decreases again as training progresses*.\n","\n","#Key Observations\n","**Double Descent**:\n","\n","* The **test loss** exhibits a double descent pattern as the model transitions from **underfitting to overfitting** and finally stabilizes.\n","\n","**Effect of Training Duration**:\n","\n","* Short training durations may lead to **underfitting**, while prolonged training can help the model generalize better."],"metadata":{"id":"nYMSoaEZ9WZs"}}]}