{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOPNpDIpHiR/UTxT4VhCead"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#1. Import Necessary Libraries"],"metadata":{"id":"9rJZGRrYWB7o"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mR2XxCTWldXA"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import Ridge\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","source":["* numpy: Used for dataset creation and numerical operations.\n","* matplotlib.pyplot: For visualizing the results (MSE and\n","$ùëÖ^2$\n","  score).\n","* PolynomialFeatures: Expands input features into polynomial terms, increasing model complexity.\n","* Ridge: Implements Ridge regression (L2 regularization) to reduce overfitting.\n","mean_squared_error and r2_score: Performance metrics for regression tasks.\n","  * MSE: Measures error magnitude.\n","\n","  * $ùëÖ^2$ : Indicates the proportion of variance explained by the model.\n","* train_test_split: Splits data into training and testing sets."],"metadata":{"id":"Ysu8X3rUoq5a"}},{"cell_type":"markdown","source":["#2. Generate Synthetic Dataset"],"metadata":{"id":"1XFq7JQxWETa"}},{"cell_type":"code","source":["# Generate synthetic dataset\n","np.random.seed(42)\n","n_samples = 30\n","X = np.random.uniform(-1, 1, size=(n_samples, 1))\n","y = np.sin(2 * np.pi * X).ravel() + 0.3 * np.random.normal(size=n_samples)"],"metadata":{"id":"100iQqZppqRx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* X: Generated as uniformly distributed points between -1 and 1.\n","* y: A sine wave with added Gaussian noise (\n","ùúé\n","= 0.3) to simulate noisy real-world data."],"metadata":{"id":"QCuRSKbKp0Xu"}},{"cell_type":"markdown","source":["#3. Initialize Parameters"],"metadata":{"id":"IwnEjSxOWHKN"}},{"cell_type":"code","source":["# Regularization parameters\n","dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.4]\n","l2_weights = [0.0, 0.1, 0.5, 1.0, 5.0]\n","\n","# Initialize results\n","degrees = np.arange(1, 50)\n","results = {(dropout_rate, l2_weight): {'train_errors': [], 'test_errors': [], 'train_r2': [], 'test_r2': []}\n","           for dropout_rate in dropout_rates for l2_weight in l2_weights}"],"metadata":{"id":"yGjSMNt2qYa_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* dropout_rates: Simulates feature randomization during training, ranging from 0.0 (no dropout) to 0.4 (higher dropout).\n","* l2_weights: L2 regularization strengths, penalizing large weights in the model.\n","* degrees: Polynomial degrees representing model complexity.\n","* results: Stores training and testing errors (MSE) and $R^2$\n","  scores for each combination of dropout rate and L2 regularization.\n"],"metadata":{"id":"8MsfGADGqd4u"}},{"cell_type":"markdown","source":["#4. Define Dropout Function"],"metadata":{"id":"rSkglWltWJ1A"}},{"cell_type":"code","source":["# Function to simulate dropout (artificially setting weights to zero)\n","def apply_dropout(data, rate):\n","    mask = np.random.binomial(1, 1 - rate, size=data.shape)\n","    return data * mask"],"metadata":{"id":"euDssC5Pq0i8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Generates a binary mask (0 or 1) using the specified dropout rate.\n","* Zeroes out a proportion of features, simulating dropout behavior."],"metadata":{"id":"Vu0Suu6Jq238"}},{"cell_type":"markdown","source":["#5. Compute Results for Dropout and L2 Regularization"],"metadata":{"id":"I3uXqOgrWMW_"}},{"cell_type":"code","source":["for dropout_rate in dropout_rates:\n","    for l2_weight in l2_weights:\n","        train_errors = []\n","        test_errors = []\n","        train_r2_scores = []\n","        test_r2_scores = []\n","\n","        # Split data\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n","\n","        for degree in degrees:\n","            poly = PolynomialFeatures(degree=degree)\n","            X_train_poly = apply_dropout(poly.fit_transform(X_train), dropout_rate)\n","            X_test_poly = apply_dropout(poly.transform(X_test), dropout_rate)\n","\n","            model = Ridge(alpha=l2_weight)  # Combine L2 and dropout\n","            model.fit(X_train_poly, y_train)\n","\n","            y_train_pred = model.predict(X_train_poly)\n","            y_test_pred = model.predict(X_test_poly)\n","\n","            train_errors.append(mean_squared_error(y_train, y_train_pred))\n","            test_errors.append(mean_squared_error(y_test, y_test_pred))\n","            train_r2_scores.append(r2_score(y_train, y_train_pred))\n","            test_r2_scores.append(r2_score(y_test, y_test_pred))\n","\n","        results[(dropout_rate, l2_weight)]['train_errors'] = train_errors\n","        results[(dropout_rate, l2_weight)]['test_errors'] = test_errors\n","        results[(dropout_rate, l2_weight)]['train_r2'] = train_r2_scores\n","        results[(dropout_rate, l2_weight)]['test_r2'] = test_r2_scores"],"metadata":{"id":"sNlrFFHqq7Gv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Outer Loops:\n","* Iterate over all combinations of dropout rates and L2 regularization values.\n","2. Inner Loop:\n","\n","Polynomial Feature Expansion:\n","* Expands features for each polynomial degree.\n","Dropout:\n","\n","* Applies random feature deactivation to simulate dropout.\n","\n","Ridge Regression:\n","\n","* Fits the model with the given L2 regularization value.\n","\n","Error and Score Calculation:\n","* Computes MSE and $ùëÖ^2$ for both training and testing datasets.\n","3. Result Storage:\n","* Errors and scores are stored for later analysis."],"metadata":{"id":"rV4qGcUOq9W1"}},{"cell_type":"markdown","source":["#6. Plot Results: Mean Squared Error (MSE)"],"metadata":{"id":"IZV-Y6QhWPCX"}},{"cell_type":"code","source":["# Plot MSE results\n","fig, axs = plt.subplots(len(l2_weights), 1, figsize=(15, 4 * len(l2_weights)), sharex=True)\n","for i, l2_weight in enumerate(l2_weights):\n","    ax = axs[i]\n","    for dropout_rate in dropout_rates:\n","        train_errors = results[(dropout_rate, l2_weight)]['train_errors']\n","        test_errors = results[(dropout_rate, l2_weight)]['test_errors']\n","\n","        ax.plot(degrees, test_errors, label=f\"Test Loss (Dropout={dropout_rate})\", marker='o')\n","        ax.plot(degrees, train_errors, linestyle='--', label=f\"Train Loss (Dropout={dropout_rate})\")\n","\n","    ax.set_yscale(\"log\")\n","    ax.set_title(f\"L2 Regularization (Alpha={l2_weight})\")\n","    ax.set_ylabel(\"MSE (Log Scale)\")\n","    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n","\n","plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=10, title=\"Legend\")\n","plt.xlabel(\"Model Complexity (Polynomial Degree)\")\n","plt.suptitle(\"Effect of Combined Dropout and L2 Regularization on Double Descent (MSE)\", fontsize=16)\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","plt.show()"],"metadata":{"id":"aZkSRM3grjgy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Plots training and testing MSE across model complexities for different combinations of dropout and L2 regularization.\n","* Logarithmic scaling highlights differences across orders of magnitude."],"metadata":{"id":"U39DcDPbro-g"}},{"cell_type":"markdown","source":["#7. Plot Results: $ùëÖ^2$ Score"],"metadata":{"id":"reCwKvEyWSol"}},{"cell_type":"code","source":["# Plot R2 results\n","fig, axs = plt.subplots(len(l2_weights), 1, figsize=(15, 4 * len(l2_weights)), sharex=True)\n","for i, l2_weight in enumerate(l2_weights):\n","    ax = axs[i]\n","    for dropout_rate in dropout_rates:\n","        train_r2 = results[(dropout_rate, l2_weight)]['train_r2']\n","        test_r2 = results[(dropout_rate, l2_weight)]['test_r2']\n","\n","        ax.plot(degrees, test_r2, label=f\"Test R¬≤ (Dropout={dropout_rate})\", marker='o')\n","        ax.plot(degrees, train_r2, linestyle='--', label=f\"Train R¬≤ (Dropout={dropout_rate})\")\n","\n","    ax.set_title(f\"L2 Regularization (Alpha={l2_weight})\")\n","    ax.set_ylabel(\"R¬≤ Score\")\n","    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n","\n","plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=10, title=\"Legend\")\n","plt.xlabel(\"Model Complexity (Polynomial Degree)\")\n","plt.suptitle(\"Effect of Combined Dropout and L2 Regularization on Double Descent (R¬≤)\", fontsize=16)\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","plt.show()"],"metadata":{"id":"8Q29_sHwryIz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Visualizes $ùëÖ^2$\n","  scores for both training and testing data.\n","* Highlights how well the model fits the data under varying conditions."],"metadata":{"id":"9dJgBgtpr0PA"}}]}