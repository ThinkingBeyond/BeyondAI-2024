import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
from clifford import Cl
from torch.utils.data import DataLoader, TensorDataset
from sklearn import datasets
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

layout, blades = Cl(2, 0)
num_blades = len(layout.blades_list)  # Should be 4 for Cl(2, 0)

blade_tuple_to_idx = {key: idx for idx, key in enumerate(layout.blades.keys())}
idx_to_blade_tuple = {idx: key for key, idx in blade_tuple_to_idx.items()}

def get_clifford_left_kernel(M, w):
    return torch.einsum('oi,ijk->ojk', w, M)  # [out_features, num_blades, num_blades]

def get_clifford_right_kernel(M, w):
    return torch.einsum('oi,kji->ojk', w, M)  # [out_features, num_blades, num_blades]

class MultiVectorEmbedding(nn.Module):
    def __init__(self, layout, input_dim):
        super().__init__()
        self.layout = layout
        self.input_dim = input_dim
        self.num_blades = len(layout.blades_list)
        self.linear = nn.Linear(input_dim, self.num_blades)

    def encode(self, x):
        # x shape: [batch_size, input_dim]
        components = self.linear(x)  # [batch_size, num_blades]
        return components

class CliffordAlgebra:
    def __init__(self, layout):
        self.layout = layout
        self.metric = layout.sig
        self.gmt = layout.gmt
        # Convert the sparse GMT to a dense PyTorch tensor
        self.cayley = torch.tensor(layout.gmt.todense(), dtype=torch.float32)
        # Create the mapping from blade tuples to indices
        blade_tuples = self.layout.bladeTupList
        self.blade_tuple_to_idx = {blade_tuples[i]: i for i in range(len(blade_tuples))}
        self.num_blades = len(blade_tuples)

    def reverse(self, mv):
        if not torch.is_tensor(mv):
            raise TypeError("Input to reverse must be a torch.Tensor")

        # Get the grades of the blades
        grades = torch.tensor([self.layout.gradeList[blade_idx] for blade_idx in range(self.num_blades)],
                              dtype=mv.dtype, device=mv.device)
        # Compute reverse signs: sign = (-1)^{r(r-1)/2}
        reverse_signs = (-1) ** ((grades * (grades - 1)) // 2)

        # Apply reverse signs
        return mv * reverse_signs

    def embed(self, tensor, blades):
        batch_shape = tensor.shape[:-1]
        mv_tensor = torch.zeros(*batch_shape, self.num_blades, device=tensor.device)
        for idx, blade_idx in enumerate(blades):
            mv_tensor[..., int(blade_idx)] = tensor[..., idx]
        return mv_tensor

    def norm(self, mv):
        return torch.sqrt(torch.sum(mv ** 2, dim=-1))

class PGAConjugateLinear(nn.Module):
    def __init__(
        self,
        in_features,
        out_features,
        algebra,
        input_blades,
        action_blades=None,
    ):
        super().__init__()
        self.input_blades = input_blades
        self.in_features = in_features
        self.out_features = out_features
        self.algebra = algebra

        if action_blades is None:
            # Use vector blades (grade-1 blades)
            action_blades = [
                self.algebra.blade_tuple_to_idx[(1,)],
                self.algebra.blade_tuple_to_idx[(2,)],
            ]
            self.action_blades = [int(idx) for idx in action_blades]
        else:
            self.action_blades = [int(idx) for idx in action_blades]

        self.n_action_blades = len(self.action_blades)

        # Parameters
        self._action = nn.Parameter(torch.empty(out_features, self.n_action_blades))  # [out_features, n_action_blades]
        self.weight = nn.Parameter(torch.empty(out_features, in_features))  # [out_features, in_features]

        self.inverse = self.algebra.reverse

        self.reset_parameters()

    def reset_parameters(self):
        torch.nn.init.uniform_(self._action, -1, 1)

        # Normalize the action
        action_mv = self.algebra.embed(self._action.data, self.action_blades)  # [out_features, num_blades]
        norm = self.algebra.norm(action_mv)  # [out_features]

        epsilon = 1e-12
        norm = norm.unsqueeze(-1)  # [out_features,1]
        self._action.data = self._action.data / (norm + epsilon)  # [out_features, n_action_blades]

        # Initialize weights
        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    @property
    def action(self):
        return self.algebra.embed(self._action, self.action_blades)  # [out_features, num_blades]

    def forward(self, input):
        # input: [batch_size, in_channels, num_blades]
        batch_size, in_channels, num_blades = input.shape
        M = self.algebra.cayley.to(input.device)  # [num_blades, num_blades, num_blades]
        k = self.action.to(input.device)  # [out_features, num_blades]
        k_ = self.inverse(k)  # [out_features, num_blades]

        # Compute k_l and k_r
        k_l = get_clifford_left_kernel(M, k)  # [out_features, num_blades, num_blades]
        k_r = get_clifford_right_kernel(M, k_)  # [out_features, num_blades, num_blades]

        # Reshape k_l and k_r for batch computation
        k_l = k_l.unsqueeze(0).repeat(batch_size * in_channels, 1, 1, 1)
        k_r = k_r.unsqueeze(0).repeat(batch_size * in_channels, 1, 1, 1)

        # Reshape x for batch computation
        x = input.view(batch_size * in_channels, 1, num_blades)  # [batch_size*in_channels, 1, num_blades]

        # Apply geometric transformations
        x = torch.einsum('bij,bijk->bik', x, k_l)  # Left multiplication
        x = torch.einsum('bij,bijk->bik', x, k_r)  # Right multiplication

        # x now has shape [batch_size * in_channels, out_channels, num_blades]
        x = x.view(batch_size, in_channels, self.out_features, num_blades)

        # Apply weights
        weight = self.weight.t().unsqueeze(0).unsqueeze(-1)  # [1, in_channels, out_features, 1]
        x = x * weight  # Element-wise multiplication

        # Sum over in_channels
        x = x.sum(dim=1)  # [batch_size, out_features, num_blades]

        return x

class MultiVectorAct(nn.Module):
    def __init__(self, channels, layout, input_blades, kernel_blades=None, agg="linear"):
        super().__init__()
        self.layout = layout
        self.input_blades = tuple(input_blades)
        if kernel_blades is not None:
            self.kernel_blades = tuple(kernel_blades)
        else:
            self.kernel_blades = self.input_blades

        self.agg = agg

    def forward(self, input):
        if self.agg == "linear":
            activation = torch.sigmoid(input)
            x = input * activation
        elif self.agg == "sum":
            activation = torch.sigmoid(input[..., self.kernel_blades].sum(dim=-1, keepdim=True))
            x = input * activation
        elif self.agg == "mean":
            activation = torch.sigmoid(input[..., self.kernel_blades].mean(dim=-1, keepdim=True))
            x = input * activation
        else:
            raise ValueError(f"Aggregation {self.agg} not implemented.")
        return x

class MSiLU(MultiVectorAct):
    def __init__(self, channels, layout):
        num_blades = len(layout.blades_list)
        super().__init__(
            channels=channels,
            layout=layout,
            input_blades=tuple(range(num_blades)),
            agg='linear'
        )

class CliffordNormalization(nn.Module):
    def __init__(self, num_features, num_blades):
        super().__init__()
        self.bn = nn.BatchNorm1d(num_features * num_blades)

    def forward(self, x):
        batch_size, channels, num_blades = x.shape
        x_flat = x.view(batch_size, channels * num_blades)
        x_norm = self.bn(x_flat)
        x_norm = x_norm.view(batch_size, channels, num_blades)
        return x_norm

class GCALayer(nn.Module):
    def __init__(self, in_channels, out_channels, algebra):
        super().__init__()
        self.algebra = algebra
        self.num_blades = self.algebra.num_blades
        self.conj_linear = PGAConjugateLinear(
            in_features=in_channels,
            out_features=out_channels,
            algebra=algebra,
            input_blades=tuple(range(self.num_blades))
        )
        self.normalization = CliffordNormalization(out_channels, self.num_blades)
        self.activation = MSiLU(channels=out_channels, layout=layout)

    def forward(self, x):
        x = self.conj_linear(x)
        x = self.normalization(x)
        x = self.activation(x)
        return x

class GCA_MLP(nn.Module):
    def __init__(self, hidden_dim=16, num_layers=3, N=3, input_dim=4):
        super().__init__()
        self.N = N
        self.algebra = CliffordAlgebra(layout)
        self.num_blades = self.algebra.num_blades
        layers = []
        in_channels = 1  # Since the input will be of shape [batch_size, 1, num_blades]
        for i in range(num_layers):
            if i == num_layers - 1:
                out_channels = 1  # Keep output as [batch_size, 1, num_blades]
            else:
                out_channels = hidden_dim
            layers.append(GCALayer(in_channels, out_channels, self.algebra))
            in_channels = out_channels
        self.layers = nn.ModuleList(layers)
        self.embedding = MultiVectorEmbedding(layout, input_dim)
        self.output_linear = nn.Linear(self.num_blades, N)  # Map multivector components to logits
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'

    def forward(self, x):
        # x shape: [batch_size, input_dim]
        x = x.view(x.size(0), -1)  # Ensure x is of shape [batch_size, input_dim]
        x = self.embedding.encode(x)  # [batch_size, num_blades]
        x = x.to(self.device)
        x = x.unsqueeze(1)  # Add channels dimension: [batch_size, 1, num_blades]
        for layer in self.layers:
            x = layer(x)
        # Output shape: [batch_size, out_channels, num_blades]
        # For the last layer, out_channels is 1
        x = x.squeeze(1)  # [batch_size, num_blades]
        logits = self.output_linear(x)  # [batch_size, N]
        return logits

# Data loading and training code
def train(model, device, train_loader, optimizer, epoch, train_losses):
    model.train()
    criterion = nn.MSELoss()
    total_loss = 0
    correct = 0
    total = 0
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device).float(), target.to(device)
        optimizer.zero_grad()
        output = model(data)  # [batch_size, N]
        # Convert targets to one-hot encoding
        target_one_hot = F.one_hot(target, num_classes=3).float()
        loss = criterion(output, target_one_hot)
        loss.backward()
        optimizer.step()
        train_losses.append(loss.item())
        total_loss += loss.item() * data.size(0)
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()
        total += data.size(0)
        if batch_idx % 10 == 0:
            print(f'Train Epoch: {epoch} \tLoss: {loss.item():.6f}')
    avg_loss = total_loss / total
    accuracy = 100. * correct / total
    return avg_loss, accuracy

def test(model, device, test_loader, test_losses):
    model.eval()
    criterion = nn.MSELoss(reduction='sum')
    test_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device).float(), target.to(device)
            output = model(data)  # [batch_size, N]
            # Convert targets to one-hot encoding
            target_one_hot = F.one_hot(target, num_classes=3).float()
            batch_loss = criterion(output, target_one_hot).item()
            test_loss += batch_loss  # Sum up batch loss
            pred = output.argmax(dim=1, keepdim=True)     # Get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()
            test_losses.append(batch_loss / len(data))  # Average loss for the batch
            total += data.size(0)
    test_loss /= len(test_loader.dataset)
    avg_loss = test_loss / total
    accuracy = 100. * correct / total
    print(f'\nTest set: Average loss: {test_loss:.4f}, '
          f'Accuracy: {correct}/{len(test_loader.dataset)}'
          f' ({100. * correct / len(test_loader.dataset):.0f}%)\n')
    return avg_loss, accuracy

def main():
    batch_size = 16
    test_batch_size = 16
    epochs = 15
    lr = 0.01
    hidden_dim = 16
    num_layers = 3

    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")

    # Load Iris Dataset
    iris = datasets.load_iris()
    X = iris.data  # Features: [150, 4]
    y = iris.target  # Labels: [150]

    # Split into training and testing datasets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Convert to PyTorch tensors
    X_train = torch.tensor(X_train, dtype=torch.float32)
    y_train = torch.tensor(y_train, dtype=torch.long)
    X_test = torch.tensor(X_test, dtype=torch.float32)
    y_test = torch.tensor(y_test, dtype=torch.long)

    # Create TensorDatasets
    train_dataset = TensorDataset(X_train, y_train)
    test_dataset = TensorDataset(X_test, y_test)

    # DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size,
                              shuffle=True, num_workers=1)
    test_loader = DataLoader(test_dataset, batch_size=test_batch_size,
                             shuffle=False, num_workers=1)

    model = GCA_MLP(hidden_dim=hidden_dim, num_layers=num_layers, N=3, input_dim=4).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # Lists to store losses
    train_losses = []
    test_losses = []
    train_accuracies = []
    test_accuracies = []

    for epoch in range(1, epochs + 1):
        train_loss, train_acc = train(model, device, train_loader, optimizer, epoch)
        test_loss, test_acc = test(model, device, test_loader)
        train_losses.append(train_loss)
        train_accuracies.append(train_acc)
        test_losses.append(test_loss)
        test_accuracies.append(test_acc)

    # Plotting
    epochs_range = range(1, epochs + 1)
    plt.figure(figsize=(12, 5))

    # Plot loss
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, train_losses, label='Train Loss')
    plt.plot(epochs_range, test_losses, label='Test Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Loss over Epochs')
    plt.legend()

    # Plot accuracy
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, train_accuracies, label='Train Accuracy')
    plt.plot(epochs_range, test_accuracies, label='Test Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.title('Accuracy over Epochs')
    plt.legend()

    plt.tight_layout()
    plt.show()

if __name__ == '__main__':
    main()
