# Define the geometric algebra parameters
layout, blades = Cl(2, 0)
num_blades = len(layout.blades_list)
g = [1, 1]  # Signature

blade_tuple_to_idx = {key: idx for idx, key in enumerate(layout.blades.keys())}
idx_to_blade_tuple = {idx: key for key, idx in blade_tuple_to_idx.items()}

def ensure_tensor(x, dtype=torch.float32, device=None):
    """Ensure input is a tensor with the correct dtype and device."""
    if not torch.is_tensor(x):
        x = torch.tensor(x, dtype=dtype)
    if device is not None:
        x = x.to(device)
    return x

def debug_shapes(**tensors):
    """Log the shapes, dtypes, and devices of the given tensors."""
    for name, tensor in tensors.items():
        print(f"{name} shape: {tensor.shape}, dtype: {tensor.dtype}, device: {tensor.device}")

def get_clifford_left_kernel(M, w):
    return torch.einsum('oi,ijk->okj', w, M)  # [out_features,4,4]

def get_clifford_right_kernel(M, w):
    return torch.einsum('oi,jik->ojk', w, M)  # [out_features,4,4]

class MultiVectorEmbedding:
    def __init__(self, layout):
        """
        Initialize the embedding with the layout of the Clifford algebra.

        Args:
            layout: Clifford algebra layout (e.g., from Cl(2, 0)).
        """
        self.layout = layout
        self.blade_tuple_to_idx = {key: idx for idx, key in enumerate(layout.blades.keys())}
        self.idx_to_blade_tuple = {idx: key for key, idx in self.blade_tuple_to_idx.items()}

    def encode(self, x):
        """
        Embed input tensor into multivector components.

        Args:
            x: Input tensor of shape (batch_size, 2), where each row corresponds
            to the features for e1 and e2.

        Returns:
            Tensor of shape (batch_size, num_blades) with components mapped to blades.
        """
        batch_size = x.shape[0]
        components = torch.zeros(batch_size, num_blades, device=x.device)

        # Map the features to vector components
        e1_idx = self.blade_tuple_to_idx['e1']
        e2_idx = self.blade_tuple_to_idx['e2']
        components[:, e1_idx] = x[:, 0]
        components[:, e2_idx] = x[:, 1]

        # **Set the scalar component to 1**
        scalar_idx = self.blade_tuple_to_idx['']
        components[:, scalar_idx] = 1.0

        return components

    def extract_output(self, mv):
        """
        Extract scalar component (grade-0 blade) as output.

        Args:
            mv: Multivector tensor of shape (batch_size, out_channels, num_blades).

        Returns:
            Tensor of shape (batch_size,) if out_channels=1,
            or (batch_size, out_channels) otherwise.
        """
        scalar_idx = self.blade_tuple_to_idx['']
        output = mv[:, :, scalar_idx]
        if output.size(1) == 1:
            output = output.squeeze(1)
        return output

class MultiVectorAct(nn.Module):
    def __init__(self, channels, layout, input_blades, kernel_blades=None, agg="linear"):
        super().__init__()
        self.layout = layout
        self.input_blades = tuple(input_blades)
        if kernel_blades is not None:
            self.kernel_blades = tuple(kernel_blades)
        else:
            self.kernel_blades = self.input_blades

        if agg == "linear":
            self.conv = nn.Conv1d(
                in_channels=channels * len(self.kernel_blades),
                out_channels=channels * len(self.input_blades),
                kernel_size=1,
                groups=channels,
            )
        self.agg = agg

    def forward(self, input):
        """
        Args:
            input: Tensor of shape [batch_size, channels, num_blades]

        Returns:
            Tensor of shape [batch_size, channels, num_blades]
        """
        if self.agg == "linear":
            v_kernel = input.view(input.size(0), -1).unsqueeze(-1)
            activation = torch.sigmoid(self.conv(v_kernel)).squeeze(-1)
            activation = activation.view(input.size(0), -1, len(self.input_blades))
            x = input * activation
        elif self.agg == "sum":
            activation = torch.sigmoid(input[..., self.kernel_blades].sum(dim=-1, keepdim=True))
            x = input * activation
        elif self.agg == "mean":
            activation = torch.sigmoid(input[..., self.kernel_blades].mean(dim=-1, keepdim=True))
            x = input * activation
        else:
            raise ValueError(f"Aggregation {self.agg} not implemented.")
        return x

class CliffordAlgebra:
    def __init__(self, layout):
        self.layout = layout
        self.metric = layout.sig
        self.gmt = layout.gmt
        # Convert the sparse GMT to a dense PyTorch tensor
        self.cayley = torch.tensor(layout.gmt.todense(), dtype=torch.float32)  # [4,4,4] float32 tensor

    def geometric_product(self, a, b):
        """Compute the geometric product of two multivectors."""
        return a * b

    def reverse(self, mv):
        """
        Compute the reverse of a multivector.

        Args:
            mv (Tensor): Multivector tensor of shape [..., num_blades]

        Returns:
            Tensor: Reversed multivector with the same shape.
        """
        if not torch.is_tensor(mv):
            raise TypeError("Input to reverse must be a torch.Tensor")

        # Get the grades of the blades
        grades = torch.tensor([self.layout.gradeList[blade_idx] for blade_idx in range(len(self.layout.blades_list))],
                              dtype=mv.dtype, device=mv.device)
        # Compute reverse signs: sign = (-1)^{r(r-1)/2}
        reverse_signs = (-1) ** ((grades * (grades - 1)) // 2)

        # Apply reverse signs
        return mv * reverse_signs

    def embed(self, tensor, blades):
        """Embed a tensor into a multivector."""
        batch_shape = tensor.shape[:-1]
        mv_tensor = torch.zeros(*batch_shape, len(self.layout.blades_list), device=tensor.device)
        for idx, blade_idx in enumerate(blades):
            mv_tensor[..., blade_idx] = tensor[..., idx]
        return mv_tensor

    def get(self, mv_tensor, blades):
        """Extract blade components from a multivector."""
        components = []
        for blade_idx in blades:
            components.append(mv_tensor[..., blade_idx])
        return torch.stack(components, dim=-1)

    def norm(self, mv):
        """Computes the norm of a multivector."""
        return torch.sqrt(torch.sum(mv ** 2, dim=-1))

class PGAConjugateLinear(nn.Module):
    def __init__(
        self,
        in_features,
        out_features,
        algebra,
        input_blades,
        action_blades=(0, 1, 2, 3),
    ):
        super().__init__()
        self.input_blades = input_blades
        self.in_features = in_features
        self.out_features = out_features
        self.algebra = algebra
        self.action_blades = action_blades
        self.n_action_blades = len(action_blades)

        # Define self._action as a 2D tensor [out_features,4]
        self._action = nn.Parameter(torch.empty(out_features, len(self.action_blades)))  # [out_features,4]
        self.weight = nn.Parameter(torch.empty(out_features, in_features))  # [out_features,4]
        self.embed_e0 = nn.Parameter(torch.zeros(1, out_features, 1))  # [1, out_features,1]

        self.inverse = self.algebra.reverse

        self.reset_parameters()

    def reset_parameters(self):
        # Initialize uniformly
        torch.nn.init.uniform_(self._action, -1, 1)

        # Normalize the action
        action_mv = self.algebra.embed(self._action.data, self.action_blades)  # [out_features,4]
        norm = self.algebra.norm(action_mv)  # [out_features]

        # Safeguard against division by zero
        epsilon = 1e-12
        norm = norm.unsqueeze(-1)  # [out_features,1]
        self._action.data = self._action.data / (norm + epsilon)  # [out_features,4]

        # Initialize weights
        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    @property
    def action(self):
        return self.algebra.embed(self._action, self.action_blades)  # [out_features,4]

    def forward(self, input):
        """
        Args:
            input: Tensor of shape [batch_size, in_features=4]

        Returns:
            Tensor of shape [batch_size, out_features, num_blades=4]
        """
        M = self.algebra.cayley  # [4,4,4]

        # Get action and inverse action
        k = self.action  # [out_features,4]
        k_ = self.inverse(k)  # [out_features,4]

        # Compute k_l and k_r using einsum
        k_l = get_clifford_left_kernel(M, k)  # [out_features,4,4]
        k_r = get_clifford_right_kernel(M, k_)  # [out_features,4,4]

        # Embed input into multivectors
        x = self.algebra.embed(input, self.input_blades)  # [batch_size,4]
        x = x.unsqueeze(1).repeat(1, self.out_features, 1)  # [batch_size, out_features,4]

        # Remove scalar component override
        # embed_e0_expanded = self.embed_e0.expand(x.size(0), -1, -1)  # [batch_size, out_features,1]
        # x = x.clone()
        # x[..., 0:1] = embed_e0_expanded[..., 0:1]  # [batch_size, out_features,1]

        # Compute scaling factors
        scaling = self.weight.unsqueeze(-1) * k_r * k_l  # [out_features,4,4]

        # Sum over the last dimension to aggregate scaling factors
        scaling = scaling.sum(dim=-1)  # [out_features,4]

        # Expand scaling to [batch_size, out_features,4]
        scaling = scaling.unsqueeze(0).expand(x.size(0), -1, -1)  # [batch_size, out_features,4]

        # Apply scaling
        x = x * scaling  # [batch_size, out_features,4]

        return x  # [batch_size, out_features,4]

class MSiLU(MultiVectorAct):
    def __init__(self, channels, layout):
        num_blades = len(layout.blades_list)
        super().__init__(
            channels=channels,
            layout=layout,
            input_blades=tuple(range(num_blades)),
            agg='linear'
        )

class CliffordNormalization(CliffordGroupNorm1d):
    def __init__(self, num_features, g):
        super().__init__(
            num_groups=1,
            channels=num_features,
            g=g
        )

class GCALayer(nn.Module):
    def __init__(self, in_channels, out_channels, algebra):
        super().__init__()
        self.conj_linear = PGAConjugateLinear(
            in_features=in_channels,
            out_features=out_channels,
            algebra=algebra,
            input_blades=tuple(range(num_blades))
        )
        self.normalization = CliffordNormalization(out_channels, g)
        self.activation = MSiLU(channels=out_channels, layout=layout)

    def forward(self, x):
        x = self.conj_linear(x)
        x = self.normalization(x)
        x = self.activation(x)
        return x

class GCA_MLP(nn.Module):
    def __init__(self, hidden_dim=4, num_layers=1):
        super().__init__()
        algebra = CliffordAlgebra(layout)  # Single instance reused across layers
        layers = []
        in_channels = num_blades  # Starting with input blades (4)
        for i in range(num_layers):
            if i == num_layers - 1:
                out_channels = 1  # Final output layer
            else:
                out_channels = hidden_dim  # Hidden layers (4)
            layers.append(GCALayer(in_channels, out_channels, algebra))
            in_channels = out_channels  # Update for next layer
        self.layers = nn.ModuleList(layers)
        self.embedding = MultiVectorEmbedding(layout)

    def forward(self, x):
        # x is a tensor of shape (batch_size, 2)
        x = self.embedding.encode(x)  # [batch_size,4]
        for layer in self.layers:
            x = layer(x)  # [batch_size, out_channels,4]
            # No flattening; pass [batch_size, out_channels,4] to next layer
        output = self.embedding.extract_output(x)  # [batch_size] or [batch_size, out_channels]
        return output

#new
class GeometricLayer(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(GeometricLayer, self).__init__()
        self.fc = nn.Linear(input_size, hidden_size)

    def forward(self, x):
        x = self.fc(x)
        return x

class GeoClassifier(nn.Module):
    def __init__(self, input_dim, output_dim, clifford_signature):
        super(GeoClassifier, self).__init__()
        self.clifford_conv = _CliffordConvNd(clifford_signature,
                                             input_dim, 64, kernel_size=[3,3],
                                             stride=1, padding=1,
                                             dilation = 1, groups = 1,
                                             bias = "False",
                                             padding_mode = "zeros")
        self.clifford_norm = _CliffordBatchNorm(g=[1, 1], channels=2,
                                                eps= 1e-5,
                                                momentum = 0.1,
                                                affine = "True",
                                                track_running_stats = "True")

        self.clifford_linear = CliffordLinear(clifford_signature, 128, 64)
        self.fc = nn.Linear(64, output_dim)

        #CliffordConvNd.__init__() missing 4 required positional arguments: 'dilation', 'groups', 'bias', and 'padding_mode'

    def forward(self, x: torch.Tensor, conv_fn=None) -> torch.Tensor:
        if conv_fn is not None:
            x = conv_fn(self.clifford_conv, x)  # Use the callable conv_fn
        else:
            x = self.clifford_conv(x)  # Default conv layer

        x = self.clifford_norm(x)
        x = self.clifford_pool(x)
        x = x.flatten(1)  # Flatten for the fully connected layer
        x = self.fc(x)
        return x
